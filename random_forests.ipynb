{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Imports and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.6.10\n",
      "Numpy version: 1.18.1\n",
      "Pandas version: 1.0.3\n",
      "SKlearn version: 0.22.1\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "from platform import python_version\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# import sklearn functions\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, SCORERS\n",
    "from sklearn.model_selection import \\\n",
    "    cross_validate, cross_val_score, cross_val_predict, GridSearchCV, \\\n",
    "    StratifiedKFold, train_test_split, StratifiedShuffleSplit, TimeSeriesSplit\n",
    "\n",
    "# apply plt style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# force jupyter to output all statements\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# printing the versions of python, numpy, pandas and sklearn\n",
    "from platform import python_version\n",
    "print('Python version: ' + python_version())\n",
    "print('Numpy version: ' + np.__version__)\n",
    "print('Pandas version: ' + pd.__version__)\n",
    "print('SKlearn version: ' + sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading already cleaned file exported from data_preparation.ipynb\n",
    "data = pd.read_csv('data/cleaned_data.csv')\n",
    "\n",
    "# loading file with only the most important features\n",
    "data_top_features = pd.read_csv('data/data_top_features.csv')\n",
    "\n",
    "# naming the data frames\n",
    "data.name, data_top_features.name = 'data', 'data_top_features'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Indexing, Sorting and Inspecting Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 906 companies and 189 variables in \"data\".\n",
      "The data frame \"data\" has 0 missing values.\n",
      "There are 301 companies and 66 variables in \"data_top_features\".\n",
      "The data frame \"data_top_features\" has 0 missing values.\n"
     ]
    }
   ],
   "source": [
    "# order splticrm variable\n",
    "splticrm_categories = pd.CategoricalDtype(categories=[\n",
    "    'CCC or lower', 'CCC+', 'B-', 'B', 'B+', 'BB-', 'BB', 'BB+', 'BBB-', 'BBB', 'BBB+',\n",
    "    'A-', 'A', 'A+', 'AA-', 'AA', 'AA+', 'AAA'], ordered=True)\n",
    "\n",
    "for df in [data, data_top_features]:\n",
    "    \n",
    "    # set multi-index for both data frames\n",
    "    df.set_index(['PERMNO', 'date'], inplace=True)\n",
    "    \n",
    "    # sort both data frames by multiindex\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Transfer ordered categories to splticrm\n",
    "    df[\"splticrm\"] = df[\"splticrm\"].astype(splticrm_categories)\n",
    "    \n",
    "    #print the dimensions for both  data sets\n",
    "    print('There are ' + str(len(set([i[0] for i in df.index.tolist()]))) + \\\n",
    "      ' companies and ' + str(df.shape[1]) + ' variables in \"' + df.name + '\".')\n",
    "    \n",
    "    # ensuring that there are no null values left in both data frames\n",
    "    if (not df.isnull().values.any()):\n",
    "        print('The data frame \"' + df.name + '\" has 0 missing values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S&P 500 observations make up 37.81 % of total observations.\n",
      "In total, there are 16639 observations of S&P500 companies.\n"
     ]
    }
   ],
   "source": [
    "# look at the SP500 companies\n",
    "SP500_percentage = round((data[data['in_sp500'] == 1].shape[0] / data.shape[0]) * 100, 2)\n",
    "print('S&P 500 observations make up ' + str(SP500_percentage) + ' % of total observations.')\n",
    "\n",
    "SP500_total = data[data['in_sp500'] == 1].shape[0]\n",
    "print('In total, there are ' + str(SP500_total) + ' observations of S&P500 companies.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Determining the Naive Classifier (Benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most frequent ratings make up 54.78 % of all ratings.\n",
      "The 10 most frequent ratings make up 90.84 % of all ratings.\n",
      "The naive classifier would always predict rating BBB.\n",
      "The share of correct predictions (benchmark) to beat is 14.11 %.\n"
     ]
    }
   ],
   "source": [
    "# determining absolute and relative frequencies\n",
    "absolute_frequencies = [r[1] for r in Counter(data.splticrm).most_common()]\n",
    "relative_frequencies = [(af / data.shape[0]) for af in absolute_frequencies]\n",
    "for nr in [5, 10]:\n",
    "    print('The ' + str(nr) + ' most frequent ratings make up ' + \\\n",
    "          str(round(sum(relative_frequencies[:nr]) * 100, 2)) + ' % of all ratings.')\n",
    "\n",
    "# determining the naive classifier\n",
    "naive_classifier = Counter(data.splticrm).most_common()[0][0]\n",
    "print('The naive classifier would always predict rating ' + naive_classifier + '.')\n",
    "\n",
    "# determine the benchmark\n",
    "total_observations = data.shape[0]\n",
    "most_frequent_response = Counter(data.splticrm).most_common()[0][1]\n",
    "\n",
    "benchmark = round(most_frequent_response / total_observations * 100, 2)\n",
    "print('The share of correct predictions (benchmark) to beat is ' + str(benchmark) + ' %.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Splitting Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering only sp500 companies\n",
    "SP500 = data[data['in_sp500'] == 1]\n",
    "\n",
    "# separate response variable\n",
    "SP500_y, SP500_X = SP500['splticrm'], SP500.drop(columns=['splticrm'])\n",
    "\n",
    "# test train split\n",
    "SP500_X_train, SP500_X_test, SP500_y_train, SP500_y_test = train_test_split(\n",
    "    SP500_X, SP500_y, test_size=0.2, random_state=21, stratify=SP500_y, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "random_forest_pipe = Pipeline([('rf', RandomForestClassifier(\\\n",
    "    random_state=21, oob_score=True))])\n",
    "\n",
    "# create k-fold object\n",
    "k_fold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# fit pipeline\n",
    "random_forest_pipe.fit(SP500_X_train, SP500_y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The oob score is 0.97.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fit_times are: [29.551, 29.629, 29.562, 29.478, 18.178]\n",
      "The score_times are: [0.475, 0.465, 0.451, 0.457, 0.258]\n",
      "The test_precision_macros are: [0.908, 0.975, 0.951, 0.963, 0.959]\n",
      "The test_recall_macros are: [0.917, 0.976, 0.93, 0.943, 0.949]\n",
      "The test_accuracys are: [0.969, 0.968, 0.967, 0.964, 0.971]\n",
      "\n",
      "The mean accuracy is 0.968 +/- 0.003\n"
     ]
    }
   ],
   "source": [
    "# looking at oob score to get a first idea of how this model generalizes\n",
    "print('The oob score is ' + str(round(random_forest_pipe['rf'].oob_score_, 2)) + '.\\n')\n",
    "\n",
    "# some multi-class metrics on the training set\n",
    "multi_class_metrics = ['precision_macro', 'recall_macro', 'accuracy']\n",
    "cv_scores = cross_validate(random_forest_pipe, SP500_X_train, SP500_y_train, cv=k_fold,\n",
    "                           scoring=multi_class_metrics, n_jobs=-1);\n",
    "\n",
    "# print the metrics\n",
    "for metric_name, metric_values in cv_scores.items():\n",
    "    print('The ' + metric_name + 's are: ' + \\\n",
    "          str([round(value, 3) for value in metric_values.tolist()]))\n",
    "\n",
    "# print the mean accuracy\n",
    "cv_mean, cv_std = cv_scores[\"test_accuracy\"].mean(), cv_scores[\"test_accuracy\"].std()\n",
    "print(f'\\nThe mean accuracy is {cv_mean:.3f} +/- {cv_std:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Parameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters to do gridsearchcv over\n",
    "max_depth = np.array([10, 30, 50, 70])\n",
    "class_weight = ['balanced', None]\n",
    "\n",
    "# Minimum number of samples required to split any internal node \n",
    "min_samples_split = np.array([1, 2, 5]) \n",
    "\n",
    "# The minimum number of samples required to be at a leaf/terminal node\n",
    "min_samples_leaf = np.array([1, 2, 5])\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {'rf__max_depth': max_depth,\n",
    "              'rf__class_weight': class_weight,\n",
    "              'rf__min_samples_split': min_samples_split,\n",
    "              'rf__min_samples_leaf': min_samples_leaf\n",
    "             }\n",
    "\n",
    "random_forest_grid_search = GridSearchCV(random_forest_pipe, param_grid=param_grid,\n",
    "                                         scoring='accuracy', cv=k_fold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the grid search\n",
    "random_forest_grid_search.fit(SP500_X_train, SP500_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cv_results_ to a .csv file\n",
    "rf_GridSearchCV_cv_results_ = pd.DataFrame(random_forest_grid_search.cv_results_)\n",
    "rf_GridSearchCV_cv_results_.to_csv('gridsearch_outputs/rf_GridSearchCV_cv_results_.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The oob score is 0.97.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create pipeline and kFold object\n",
    "pipe_final = Pipeline([('rf', RandomForestClassifier(random_state=21, oob_score=True,\n",
    "    class_weight=None, max_depth=30, min_samples_leaf=1, min_samples_split=2))])\n",
    "\n",
    "k_fold = StratifiedKFold(n_splits=5, random_state=21)\n",
    "\n",
    "_ = pipe_final.fit(SP500_X_train, SP500_y_train);\n",
    "\n",
    "# looking at oob score\n",
    "print('The oob score is ' + str(round(pipe_final['rf'].oob_score_, 2)) + '.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Test Set Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy of our final model is 97.175 %\n"
     ]
    }
   ],
   "source": [
    "# predict test data\n",
    "SP500_y_test_pred = pipe_final.predict(SP500_X_test)\n",
    "\n",
    "# accuracy of final prediction on test data\n",
    "print('The prediction accuracy of our final model is ' + \\\n",
    "     str(round(accuracy_score(SP500_y_test, SP500_y_test_pred) * 100, 3)) + ' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.3 % of the false predictions are only one category off.\n",
      "11.7 % of the false predictions are two categories off.\n",
      "0.0 % of the false predictions are more than two categories off.\n"
     ]
    }
   ],
   "source": [
    "# create a list containing tuples for true and predicted ratings\n",
    "prediction_accuracies = []\n",
    "for prediction in range(SP500_y_test.shape[0]):\n",
    "    if SP500_y_test.tolist()[prediction] != SP500_y_test_pred.tolist()[prediction]:\n",
    "        prediction_accuracies.append([SP500_y_test.tolist()[prediction],\n",
    "                                     SP500_y_test_pred.tolist()[prediction]])\n",
    "\n",
    "# get an ordered list for all the ratings\n",
    "rating_categories = list(splticrm_categories.categories)\n",
    "\n",
    "# checking how far off the predictors are\n",
    "off_by_one, off_by_two, off_more = 0, 0, 0\n",
    "for tup in prediction_accuracies:\n",
    "    if abs(rating_categories.index(tup[0]) - rating_categories.index(tup[1])) < 2:\n",
    "        off_by_one += 1\n",
    "    elif abs(rating_categories.index(tup[0]) - rating_categories.index(tup[1])) < 3:\n",
    "        off_by_two += 1\n",
    "    else:\n",
    "        off_more += 1\n",
    "\n",
    "# get relative values\n",
    "off_by_one_relative = round(off_by_one / len(prediction_accuracies) * 100, 2)\n",
    "off_by_two_relative = round(off_by_two / len(prediction_accuracies) * 100, 2)\n",
    "off_more_relative = round(off_more / len(prediction_accuracies) * 100, 2)\n",
    "\n",
    "print(str(off_by_one_relative) + ' % of the false predictions are only one category off.')\n",
    "print(str(off_by_two_relative) + ' % of the false predictions are two categories off.')\n",
    "print(str(off_more_relative) + ' % of the false predictions are more than two categories off.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "206.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
